#!/usr/bin/env python3

import abc
import argparse
import dataclasses
import errno
import fcntl
import getpass
import json
import multiprocessing
import os
import pathlib
import select
import shutil
import socket
import subprocess
import tempfile
import time
import uuid
from shlex import split

import diskcache
import git
import jinja2
import numpy
import paramiko
import pint
import rich
from rich.layout import Layout
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    MofNCompleteColumn,
    Progress,
    SpinnerColumn,
    TimeElapsedColumn,
)
from rich.table import Table

title = """


     _      __ _                     _
    â”‚ â”‚    â•± _â”‚ â”‚                   â”‚ â”‚
    â”‚ â”‚__ â”‚ â”‚_â”‚ â”‚__   ___ _ __   ___â”‚ â”‚__   ___ _ __
    â”‚ '_ â•²â”‚  _â”‚ '_ â•² â•± _ â•² '_ â•² â•± __â”‚ '_ â•² â•± _ â•² '__â”‚
    â”‚ â”‚_) â”‚ â”‚ â”‚ â”‚_) â”‚  __â•± â”‚ â”‚ â”‚ (__â”‚ â”‚ â”‚ â”‚  __â•± â”‚
    â”‚_.__â•±â”‚_â”‚ â”‚_.__â•± â•²___â”‚_â”‚ â”‚_â”‚â•²___â”‚_â”‚ â”‚_â”‚â•²___â”‚_â”‚

    Benchmarking tool for bpfilter

"""


DEFAULT_FIRST_COMMIT_REF = "HEAD~10"
DEFAULT_LAST_COMMIT_REF = "wip"
DEFAULT_SOURCE_PATH = pathlib.Path(".")
DEFAULT_CACHE_PATH = pathlib.Path(".cache/bfbencher")
DEFAULT_REPORT_PATH = pathlib.Path("build/doc/html/external/benchmarks/index.html")
DEFAULT_USERNAME = getpass.getuser()
DEFAULT_REPORT_TEMPLATE_PATH = pathlib.Path("tools/benchmarks/results.html.j2")
DEFAULT_HOST = [socket.gethostname(), "localhost"]

ureg: pint.UnitRegistry = pint.UnitRegistry()


class Stats:
    def __init__(self):
        self.n_successes = 0
        self.n_failures = 0
        self.n_cache_hits = 0
        self.n_cache_misses = 0

    def success(self, from_cache: bool = False) -> None:
        if from_cache:
            self.n_cache_hits += 1
        else:
            self.n_cache_misses += 1
        self.n_successes += 1

    def failure(self, from_cache: bool = False) -> None:
        if from_cache:
            self.n_cache_hits += 1
        else:
            self.n_cache_misses += 1
        self.n_failures += 1


@dataclasses.dataclass
class TermStats:
    """Statistics for a single term (e.g., 5 or 15 commits)."""

    pct_change: float
    is_significant: bool
    mean: float
    noise: float


@dataclasses.dataclass
class BenchmarkRow:
    """Prepared data for a single benchmark row."""

    name: str
    label: str
    time_str: str
    insn_str: str | None
    terms: dict[int, dict[str, TermStats | None]]  # term -> {"time": ..., "nInsn": ...}
    runtime_ns: float = 0  # Runtime in nanoseconds for sorting
    insn_count: int = 0  # Instruction count for sorting


class Renderer(abc.ABC):
    def __init__(self) -> None:
        self._console = rich.console.Console()

    @abc.abstractmethod
    def log(self, message: str) -> None:
        pass

    @abc.abstractmethod
    def set_commits(self, commits: list[git.Commit]) -> None:
        pass

    @abc.abstractmethod
    def update_progress(self) -> None:
        pass

    @abc.abstractmethod
    def set_current_commit(self, commit: git.Commit, stats: Stats) -> None:
        pass

    @property
    def console(self) -> rich.console.Console:
        return self._console

    def _prepare_benchmark_rows(
        self, benchmarks: list["Benchmark"], terms: list[int]
    ) -> list[BenchmarkRow]:
        """Prepare benchmark data for rendering in any format."""
        rows = []
        for benchmark in benchmarks:
            last = benchmark.last
            if not last:
                continue

            time_str = f"{last.time:~.2f}"
            insn_str = f"{last.nInsn:.0f}" if last.nInsn else None
            runtime_ns = last.time.to("ns").magnitude
            insn_count = last.nInsn or 0

            term_data = {}
            for term in terms:
                bench_stats = benchmark.get_stats(term)
                if bench_stats:
                    time_analyzer = bench_stats["time"]
                    insn_analyzer = bench_stats["nInsn"]
                    term_data[term] = {
                        "time": TermStats(
                            pct_change=time_analyzer.pct_change,
                            is_significant=time_analyzer.is_significant,
                            mean=time_analyzer.mean,
                            noise=time_analyzer.noise,
                        ),
                        "nInsn": TermStats(
                            pct_change=insn_analyzer.pct_change,
                            is_significant=insn_analyzer.is_significant,
                            mean=insn_analyzer.mean,
                            noise=insn_analyzer.noise,
                        )
                        if insn_analyzer
                        else None,
                    }
                else:
                    term_data[term] = {"time": None, "nInsn": None}

            rows.append(
                BenchmarkRow(
                    name=benchmark.name,
                    label=benchmark.label,
                    time_str=time_str,
                    insn_str=insn_str,
                    terms=term_data,
                    runtime_ns=runtime_ns,
                    insn_count=insn_count,
                )
            )

        return rows

    def _get_stats_table(
        self, benchmarks: list["Benchmark"], terms: list[int] | None = None
    ) -> Table:
        """Build a Rich Table from benchmark data."""
        if terms is None:
            terms = [20]

        def format_delta(term_stats: TermStats | None) -> str:
            if not term_stats:
                return ""
            pct = term_stats.pct_change
            if term_stats.is_significant:
                color = "green" if pct < 0 else "red"
            else:
                color = "white"
            return f"[{color}]{pct:+.1f}%[/{color}]"

        rows = self._prepare_benchmark_rows(benchmarks, terms)

        summary_table = Table(title="Benchmark Summary", show_header=True)
        summary_table.add_column("Benchmark", style="cyan")
        summary_table.add_column("Time", justify="right")
        summary_table.add_column("Instructions", justify="right")
        for term in terms:
            summary_table.add_column(f"Î” Time ({term})", justify="right")
            summary_table.add_column(f"Î” Insn ({term})", justify="right")

        for row in rows:
            columns = [row.name, row.time_str, row.insn_str or "-"]
            for term in terms:
                term_data = row.terms.get(term, {"time": None, "nInsn": None})
                columns.append(format_delta(term_data["time"]))
                columns.append(format_delta(term_data["nInsn"]))
            summary_table.add_row(*columns)

        return summary_table

    def _get_context_table(
        self,
        stats: Stats,
        first_commit: git.Commit | None = None,
        last_commit: git.Commit | None = None,
        current_commit: git.Commit | None = None,
        n_commits: int | None = None,
    ) -> Table:
        table = Table(title="Context", show_header=False, box=None)
        table.add_row("", "")

        table.add_row("[bold]Commits", None)
        if first_commit:
            table.add_row(
                "â”œâ”€â”€ First",
                f"[bold]{first_commit.summary} ({first_commit.hexsha[:7]})[/bold]"
                if first_commit
                else "",
            )
        if last_commit:
            table.add_row(
                "â”œâ”€â”€ Last",
                f"[bold]{last_commit.summary} ({last_commit.hexsha[:7]})[/bold]"
                if last_commit
                else "",
            )
        if current_commit:
            table.add_row(
                "â”œâ”€â”€ Current",
                f"[bold]{current_commit.summary} ({current_commit.hexsha[:7]})[/bold]"
                if current_commit
                else "",
            )
        table.add_row("â””â”€â”€ Count", str(n_commits) if n_commits else "unknown")
        table.add_row("", "")

        table.add_row("[bold]Results", None)
        table.add_row("â”œâ”€â”€ Success", str(stats.n_successes))
        table.add_row("â””â”€â”€ Failure", str(stats.n_failures))
        table.add_row("", "")

        table.add_row("[bold]Cache", None)
        table.add_row("â”œâ”€â”€ Hits", str(stats.n_cache_hits))
        table.add_row("â””â”€â”€ Misses", str(stats.n_cache_misses))

        return table


renderer: Renderer = None


class ConsoleRenderer(Renderer):
    def __init__(self) -> None:
        super().__init__()

    def log(self, message: str) -> None:
        self._console.log(message)

    def set_commits(self, commits: list[git.Commit]) -> None:
        pass

    def update_progress(self) -> None:
        pass

    def set_current_commit(self, commit: git.Commit, stats: Stats) -> None:
        pass

    def print_summary(self, stats: Stats, benchmarks: list["Benchmark"] = []) -> None:
        self._console.log("\n")
        self._console.log(self._get_context_table(stats))
        self._console.log()
        self._console.log(self._get_stats_table(benchmarks))


class TuiRenderer(Renderer):
    def __init__(self) -> None:
        super().__init__()

        self._logs = []
        self.layout = Layout()
        self._first_commit = None
        self._last_commit = None
        self._current_commit = None
        self._n_commits = 0

        self.layout.split_row(
            Layout(name="left_pane", size=80), Layout(name="right_pane")
        )

        self.layout["left_pane"].split_column(
            Layout(name="title", size=12),
            Layout(name="progress", size=3),
            Layout(name="configuration"),
        )

        self.layout["right_pane"].split_column(
            Layout(name="header", size=4),
            Layout(name="logs", size=self._console.size.height - 4),
        )

        self.layout["header"].update("")
        self.layout["title"].update(title)

        # Update layout with content
        self._redraw_configuration()

        self._progress = Progress(
            SpinnerColumn(),
            TimeElapsedColumn(),
            MofNCompleteColumn(),
            BarColumn(bar_width=None),
            console=self._console,
        )
        self._task = self._progress.add_task("Processing...", total=100)
        self.layout["progress"].update(Panel(self._progress, border_style="green"))

    def _redraw_configuration(self, stats: Stats = Stats()):
        table = Table(show_header=False, box=None)
        table.add_row("", "")

        table.add_row("[bold]Commits", None)
        table.add_row(
            "â”œâ”€â”€ First",
            f"[bold]{self._first_commit.summary} ({self._first_commit.hexsha[:7]})[/bold]"
            if self._first_commit
            else "",
        )
        table.add_row(
            "â”œâ”€â”€ Last",
            f"[bold]{self._last_commit.summary} ({self._last_commit.hexsha[:7]})[/bold]"
            if self._last_commit
            else "",
        )
        table.add_row(
            "â”œâ”€â”€ Current",
            f"[bold]{self._current_commit.summary} ({self._current_commit.hexsha[:7]})[/bold]"
            if self._current_commit
            else "",
        )
        table.add_row("â””â”€â”€ Count", str(self._n_commits))
        table.add_row("", "")

        table.add_row("[bold]Results", None)
        table.add_row("â”œâ”€â”€ Success", str(stats.n_successes))
        table.add_row("â””â”€â”€ Failure", str(stats.n_failures))
        table.add_row("", "")

        table.add_row("[bold]Cache", None)
        table.add_row("â”œâ”€â”€ Hits", str(stats.n_cache_hits))
        table.add_row("â””â”€â”€ Misses", str(stats.n_cache_misses))

        self.layout["configuration"].update(
            Panel(table, title="Configuration", border_style="cyan")
        )

    def log(self, message: str):
        self._logs.append(message)
        height = self.layout["logs"].size - 4
        self.layout["logs"].update(
            Panel("\n".join(self._logs[-height:]), title="Logs", border_style="yellow")
        )

    def set_commits(self, commits: list[git.Commit]) -> None:
        self._first_commit = commits[0] if commits else "No commit"
        self._last_commit = commits[-1] if commits else "No commit"
        self._redraw_configuration()

        self._n_commits = len(commits)
        self._progress.update(self._task, total=self._n_commits)

    def set_current_commit(self, commit: git.Commit, stats: Stats) -> None:
        self._current_commit = commit
        self._redraw_configuration(stats)

    def update_progress(self):
        self._progress.advance(self._task, 1)

    def print_summary(self, stats: Stats, benchmarks: list["Benchmark"] = []) -> None:
        self._console.clear()
        self._console.print(title)
        self._console.print(
            self._get_context_table(
                stats, self._first_commit, self._last_commit, n_commits=self._n_commits
            )
        )
        self._console.line(1)
        self._console.print(self._get_stats_table(benchmarks))


class Analyzer:
    def __init__(
        self, last_result: float, results: list[float], threshold: float = 2.5
    ):
        # Use robust statistics (median/MAD) to handle outliers
        self.mean = numpy.median(results)
        mad = numpy.median(numpy.abs(results - self.mean))
        # Scale MAD to be comparable to std for normal distributions
        self.std = mad * 1.4826 if mad else 0
        self.z_score = (last_result - self.mean) / self.std if self.std else 0
        self.noise = self.std / self.mean if self.mean else 0
        self.pct_change = (
            ((last_result - self.mean) / self.mean) * 100 if self.mean else 0
        )
        self.is_significant = (
            abs(self.z_score) > threshold and abs(self.pct_change) > 1.0
        )


class Result:
    @classmethod
    def from_json(cls, commit: git.Commit, json: dict) -> "Result":
        return Result(
            commit,
            json["name"],
            json["iterations"],
            json["real_time"] * ureg(json["time_unit"]),
            json["cpu_time"] * ureg(json["time_unit"]),
            json.get("nInsn", 0),
            json.get("label", ""),
        )

    def __init__(
        self,
        commit: git.Commit,
        benchmark_name: str,
        iterations: int,
        real_time: pint.Quantity,
        cpu_time: pint.Quantity,
        n_insn: int,
        label: str,
    ):
        self.commit_sha = commit.hexsha
        self.commit_summary = commit.summary
        self.benchmark_name = benchmark_name
        self.iterations = iterations
        self.real_time = real_time
        self.cpu_time = cpu_time
        self.nInsn = n_insn
        self.label = label

    @property
    def short_commit_sha(self) -> str:
        return self.commit_sha[:7]

    @property
    def time(self) -> float:
        return self.cpu_time.to_compact()


class Benchmark:
    def __init__(self, name: str):
        self._name = name
        self._results: list[Result] = []
        self._label = ""

    def add_result(self, result: Result) -> None:
        self._results.append(result)
        self._label = result.label

    @property
    def name(self) -> str:
        return self._name

    @property
    def label(self) -> str:
        return self._label

    def get_stats(self, n: int):
        if len(self._results) < n + 1:
            return None

        times = [x.time.to("nanoseconds").magnitude for x in self._results]
        nInsns = self.nInsns

        return {
            "time": Analyzer(times[-1], times[-n - 1 : -1], 2.5),
            "nInsn": Analyzer(nInsns[-1], nInsns[-n - 1 : -1], 2.5) if nInsns else None,
        }

    @property
    def last(self) -> Result | None:
        return self._results[-1] if self._results else None

    @property
    def times(self) -> list[int]:
        return [x.time.to(ureg.ns).magnitude for x in self._results]

    @property
    def nInsns(self) -> list[int] | None:
        nInsns = [x.nInsn for x in self._results]
        if None not in nInsns:
            return nInsns
        return None

    @property
    def commits_sha(self) -> list[str]:
        return [x.commit_sha for x in self._results]

    @property
    def short_commits_sha(self) -> list[str]:
        return [x.short_commit_sha for x in self._results]

    @property
    def commit_subjects(self) -> list[str]:
        return [x.commit_summary for x in self._results]


class History:
    def __init__(self):
        self._benchmarks: dict[str, Benchmark] = {}
        self._last_order: list[str] = []

    def add_results(self, results: list[Result]) -> None:
        order: list[str] = []

        for result in results:
            benchmark_name = result.benchmark_name

            # If the benchmark doesn't exist yet, create it
            if benchmark_name not in self._benchmarks:
                self._benchmarks[benchmark_name] = Benchmark(benchmark_name)

            self._benchmarks[benchmark_name].add_result(result)
            order.append(benchmark_name)

        self._last_order = order

    @property
    def benchmarks(self):
        return self._benchmarks

    def sorted_benchmarks(self) -> list[Benchmark]:
        return [self._benchmarks[name] for name in self._last_order]


class Executor:
    """
    Context to execute the benchmark.

    The Executor transparently handle local or remote commands.
    """

    def __init__(self, args: argparse.Namespace):
        self._host: str = args.host
        self._workdir: pathlib.Path = (
            pathlib.Path(tempfile.gettempdir()) / f"bpfilter-{uuid.uuid4().hex[:8]}"
        )
        self._local_workdir: pathlib.Path = self._workdir
        self._cache_dir: pathlib.Path = args.cache_dir
        self._stats = Stats()
        self._cache = diskcache.Cache(self._cache_dir)
        self._local_workdir.mkdir()
        self._lock_file = None
        self._lock_channel = None

        """
        Remote hosts requires extra setup:
        - Connect to the host, using the corresponding key in ssh-agent
        - Create a work directory
        - Mount the remote host's work directory locally
        """
        if self.is_remote:
            renderer.log(f"Connecting to remote host {self._host}")
            self._remote_workdir = self._workdir
            self._agent: paramiko.Agent = paramiko.Agent()

            self._client = paramiko.SSHClient()
            self._client.set_missing_host_key_policy(paramiko.WarningPolicy())

            for key in self._agent.get_keys():
                if self._host.lower() in key.comment.lower():
                    pkey = key
                    break
            else:
                raise RuntimeError(f"No SSH agent key found matching '{self._host}'")

            self._client.connect(
                self._host,
                username=DEFAULT_USERNAME,
                pkey=pkey,
                allow_agent=False,
            )

            self.run("hostname")

            # From now on, all self.run() commands are run on the remote host

            self.run(f"mkdir -p {self._remote_workdir}")
            r = self.run(
                f"sshfs {DEFAULT_USERNAME}@{DEFAULT_HOST[0]}:{self._local_workdir} {self._remote_workdir} -C -o allow_other,default_permissions,exec,user,identityfile=~/.ssh/id_ed25519"
            )
            if r:
                raise RuntimeError(
                    f"Failed to mount local workdir on remote host (sshfs exit code: {r})"
                )

        # Acquire lock after SSH setup (so remote locks work)
        if args.lock_file:
            self._acquire_lock(args.lock_file, args.lock_wait_for)

    def _acquire_lock(self, lock_path: pathlib.Path, wait_timeout: int) -> None:
        """Acquire an exclusive lock, either locally or on the remote host."""
        if self.is_remote:
            # For remote hosts, open an SSH channel that holds the lock.
            # flock will acquire the lock, then cat blocks on stdin keeping
            # the channel (and lock) open until we close it.
            renderer.log(f"Acquiring remote lock: {lock_path}")

            # Poll with --nonblock until we get the lock or timeout.
            # We can't use --wait because we can't distinguish between
            # "flock is waiting" and "flock acquired, cat is blocking".
            waited = 0
            while True:
                self._lock_channel = self._client.get_transport().open_session()
                # Ensure the lock file exists and try to set world-rw permissions.
                # chmod may fail if we don't own the file, but that's ok - flock
                # opens read-only by default and only needs read permission.
                flock_cmd = f"touch {lock_path}; chmod 666 {lock_path} 2>/dev/null; flock --exclusive --nonblock {lock_path} cat"
                self._lock_channel.exec_command(flock_cmd)

                # Give flock a moment to fail if the lock is busy
                time.sleep(0.5)

                if self._lock_channel.exit_status_ready():
                    status = self._lock_channel.recv_exit_status()
                    self._lock_channel.close()
                    self._lock_channel = None

                    if status == 0:
                        # Shouldn't happen - cat should block, not exit with 0
                        renderer.log("Remote lock channel closed unexpectedly")
                        raise SystemExit(1)

                    # Lock is busy, retry or fail
                    if waited >= wait_timeout:
                        renderer.log(
                            f"Another instance is running (lock file: {lock_path})"
                        )
                        raise SystemExit(1)

                    renderer.log(f"Waiting for remote lock... {waited}/{wait_timeout}s")
                    time.sleep(0.5)  # Total ~1s per iteration
                    waited += 1
                else:
                    # No exit status = flock acquired, cat is blocking
                    break

            renderer.log("Remote lock acquired")
        else:
            # Local lock using fcntl.flock
            renderer.log(f"Acquiring local lock: {lock_path}")
            try:
                # Try to create with world-rw permissions
                fd = os.open(lock_path, os.O_RDWR | os.O_CREAT, 0o666)
                # If we created it, ensure permissions are set (umask may restrict)
                os.fchmod(fd, 0o666)
                self._lock_file = os.fdopen(fd, "r+")
            except PermissionError:
                # File exists but owned by another user - open read-only
                # flock works on any file descriptor, even read-only
                fd = os.open(lock_path, os.O_RDONLY)
                self._lock_file = os.fdopen(fd, "r")

            waited = 0
            while True:
                try:
                    fcntl.flock(self._lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)
                    break
                except BlockingIOError:
                    if waited >= wait_timeout:
                        renderer.log(
                            f"Another instance is running (lock file: {lock_path})"
                        )
                        self._lock_file.close()
                        self._lock_file = None
                        raise SystemExit(1)
                    renderer.log(f"Waiting for lock... {waited}/{wait_timeout}s")
                    time.sleep(1)
                    waited += 1

            renderer.log("Local lock acquired")

    def __enter__(self) -> "Executor":
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self._cache.close()

        # Release lock before cleanup
        if self._lock_channel:
            renderer.log("Releasing remote lock")
            self._lock_channel.close()
            self._lock_channel = None
        if self._lock_file:
            renderer.log("Releasing local lock")
            fcntl.flock(self._lock_file, fcntl.LOCK_UN)
            self._lock_file.close()
            self._lock_file = None

        if self.is_remote:
            self.run(f"umount -l {self._remote_workdir}")
            self.run(f"rm -rf {self._remote_workdir}")
            self._client.close()
            self._agent.close()

        shutil.rmtree(self._local_workdir)

    @property
    def host(self) -> str:
        return self._host

    @property
    def srcdir(self) -> pathlib.Path:
        return self._local_workdir / "bpfilter"

    @property
    def cache(self) -> diskcache.Cache:
        return self._cache

    @property
    def is_remote(self) -> bool:
        return self._host not in DEFAULT_HOST

    @property
    def stats(self) -> Stats:
        return self._stats

    def get_builddir(self, commit) -> pathlib.Path:
        return self._local_workdir / commit.hexsha

    def run(
        self, cmd: str, timeout: int | None = None, force_local: bool = False
    ) -> int:
        cmd = cmd.replace("{WORKDIR}", str(self._workdir))

        renderer.log(f"Running: {cmd}")

        if self.is_remote and not force_local:
            channel = self._client.get_transport().open_session()
            channel.set_combine_stderr(True)
            channel.settimeout(timeout)

            try:
                channel.exec_command(cmd)
                while data := channel.recv(1024):
                    for line in data.decode("utf-8").splitlines():
                        renderer.log(line)
            except socket.timeout:
                renderer.log(f"Command timed out after {timeout}s")
                return errno.ETIMEDOUT
            except KeyboardInterrupt:
                # Forward Ctrl+C to the process
                channel.send("\x03")
                channel.close()
                raise

            # Ensure the process is terminated
            channel.recv_exit_status()

            return channel.exit_status
        else:
            p = subprocess.Popen(
                split(cmd),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
            )

            start_time = time.time()

            while True:
                if timeout and (time.time() - start_time) > timeout:
                    p.kill()
                    p.wait()
                    renderer.log(f"Command timed out after {timeout}s")
                    return errno.ETIMEDOUT

                ready, _, _ = select.select([p.stdout], [], [], 0.1)
                if ready:
                    line = p.stdout.readline()
                    if not line:
                        break
                    line = line.rstrip()
                    if line:
                        renderer.log(line)
                elif p.poll() is not None:
                    break

            p.wait()
            return p.returncode if p.returncode is not None else 1


class FilesystemSource:
    def __init__(self, path: str, local_src_dir: pathlib.Path) -> None:
        self._path = pathlib.Path(path)
        self._local = local_src_dir

        shutil.copytree(self._path, self._local, dirs_exist_ok=True)
        self._repo: git.Repo = git.Repo(self._local)

    def _commit_wip(self) -> git.Commit:
        """Commit uncommitted changes as WIP and return the commit."""
        if self._repo.is_dirty(untracked_files=True):
            renderer.log("Committing uncommitted changes as WIP")
            self._repo.git.add(A=True)
            self._repo.index.commit("bfbencher: WIP")
        return self._repo.head.commit

    def prepare(
        self,
        since: str,
        until: str,
        include: list[str] | None = None,
        retry: list[str] | None = None,
    ) -> tuple[list[git.Commit], set[str]]:
        """
        Prepare the source repository and return commits to benchmark.

        Args:
            since: Oldest commit ref, or "wip" for uncommitted changes
            until: Newest commit ref, or "wip" for uncommitted changes
            include: Extra commit refs to include, supports "wip"
            retry: Commit refs to retry (ignore cache for these)

        Returns:
            Tuple of (commits in topological order, set of retry commit SHAs)
        """

        include = include or []
        retry = retry or []
        has_wip = (
            since.lower() == "wip"
            or until.lower() == "wip"
            or any(ref.lower() == "wip" for ref in include)
        )

        # Resolve non-wip refs to SHAs BEFORE committing WIP (so HEAD refers to
        # the original HEAD, not the WIP commit)
        since_sha = None if since.lower() == "wip" else self._repo.git.rev_parse(since)
        until_sha = None if until.lower() == "wip" else self._repo.git.rev_parse(until)
        include_shas = [
            None if ref.lower() == "wip" else self._repo.git.rev_parse(ref)
            for ref in include
        ]
        retry_shas = set()
        for ref in retry:
            if ref in ("all", "failed"):
                continue
            try:
                retry_shas.add(self._repo.git.rev_parse(ref))
            except git.exc.GitCommandError:
                renderer.log(f"Warning: could not resolve retry ref '{ref}'")

        # Handle uncommitted changes
        if self._repo.is_dirty(untracked_files=True):
            if has_wip:
                self._commit_wip()
            else:
                renderer.log("Discarding uncommitted changes in source directory")
                self._repo.git.reset("--hard", "HEAD")
                self._repo.git.clean("-fd")

        # Resolve refs (wip -> HEAD which now points to the WIP commit)
        since_ref = since_sha or self._repo.head.commit.hexsha
        until_ref = until_sha or self._repo.head.commit.hexsha

        # Get commits in range
        commits = list(
            self._repo.iter_commits(f"{since_ref}^..{until_ref}", reverse=True)
        )
        commit_set = {c.hexsha for c in commits}

        # Process included commits (use pre-resolved SHAs)
        for ref, sha in zip(include, include_shas):
            # sha is None for "wip" refs, use HEAD (now pointing to WIP commit)
            commit_sha = sha or self._repo.head.commit.hexsha
            commit = self._repo.commit(commit_sha)

            if commit.hexsha not in commit_set:
                commits.append(commit)
                commit_set.add(commit.hexsha)
                renderer.log(f"Including commit {commit.hexsha[:7]}: {commit.summary}")
            else:
                renderer.log(f"Commit {ref} already in range, skipping")

        # Sort commits in topological order (oldest first)
        if len(commits) > 1:
            all_shas = [c.hexsha for c in commits]
            ordered_shas = self._repo.git.rev_list(
                "--topo-order", "--reverse", *all_shas, "--"
            ).splitlines()
            sha_to_commit = {c.hexsha: c for c in commits}
            commits = [
                sha_to_commit[sha] for sha in ordered_shas if sha in sha_to_commit
            ]

        if include:
            renderer.log(
                f"Found {len(commits)} commits ({since_ref}..{until_ref} + {len(include)} included)"
            )
        else:
            renderer.log(f"Found {len(commits)} commits ({since_ref}..{until_ref})")

        return commits, retry_shas

    @property
    def repo(self) -> git.Repo:
        return self._repo


def has_significant_change(
    benchmark: Benchmark, terms: list[int], direction: str = "any"
) -> bool:
    """Check if a benchmark has any statistically significant change.

    Args:
        benchmark: The benchmark to check.
        terms: List of term lengths to check.
        direction: One of "any", "better", or "worse".
            - "any": any significant change
            - "better": significant improvement (negative pct_change)
            - "worse": significant regression (positive pct_change)
    """
    for term in terms:
        stats = benchmark.get_stats(term)
        if not stats:
            continue
        for analyzer in [stats["time"], stats["nInsn"]]:
            if not analyzer or not analyzer.is_significant:
                continue
            if direction == "any":
                return True
            elif direction == "better" and analyzer.pct_change < 0:
                return True
            elif direction == "worse" and analyzer.pct_change > 0:
                return True
    return False


def run_benchmarks(args: argparse.Namespace) -> History:
    executor = Executor(args)
    results = History()

    def run(executor: Executor):
        with executor:
            renderer.log(f'Preparing sources from "{args.sources}"')
            source = FilesystemSource(args.sources, executor.srcdir)
            commits, retry_shas = source.prepare(
                since=args.since,
                until=args.until,
                include=args.include,
                retry=args.retry,
            )

            if not commits:
                renderer.log("No commits found in the specified range")
                return

            renderer.set_commits(commits)

            retry_all = "all" in args.retry
            retry_failed = "failed" in args.retry

            for commit in commits:
                short_sha = source.repo.git.rev_parse(commit, short=True)
                renderer.set_current_commit(commit, executor.stats)

                cache_key = f"{commit.hexsha}-{args.host}"
                skip_cache = retry_all or commit.hexsha in retry_shas
                cached_data = None if skip_cache else executor.cache.get(cache_key, {})

                # Handle the cache, restore data if possible
                if cached_data and cached_data.get("success", False):
                    renderer.log(f"\\[{short_sha}] Using cached results")
                    executor.stats.success(from_cache=True)
                    results.add_results(
                        [
                            Result.from_json(commit, raw)
                            for raw in cached_data["results"]
                        ]
                    )
                    renderer.update_progress()
                    continue
                elif cached_data and not retry_failed:
                    renderer.log(f"\\[{short_sha}] Skipping (cached failure)")
                    executor.stats.failure(from_cache=True)
                    renderer.update_progress()
                    continue

                renderer.log(f"ðŸ”¨ \\[{short_sha}] {commit.summary}")

                # No cached data, checkout and run the benchmark
                source.repo.git.checkout(commit)

                builddir = executor.get_builddir(commit)
                benchmark_result_path = builddir / "results.json"

                renderer.log(f"\\[{short_sha}] Configuring")
                r = executor.run(
                    f"cmake -S {{WORKDIR}}/bpfilter -B {{WORKDIR}}/{commit.hexsha}"
                )
                if r:
                    renderer.log(f"âŒ \\[{short_sha}] Configure failed")
                    executor.stats.failure(from_cache=False)
                    executor.cache[cache_key] = {"success": False}
                    renderer.update_progress()
                    continue

                # Wait for bpfilter to start
                time.sleep(1)

                renderer.log(f"\\[{short_sha}] Building benchmarks")
                r = executor.run(
                    f"make -C {{WORKDIR}}/{commit.hexsha} -j {multiprocessing.cpu_count()} bpfilter bfcli benchmark_bin",
                    timeout=300,
                )
                if r:
                    renderer.log(f"âŒ \\[{short_sha}] Build failed")
                    executor.stats.failure(from_cache=False)
                    executor.cache[cache_key] = {"success": False}
                    renderer.update_progress()
                    continue

                renderer.log(f"\\[{short_sha}] Starting daemon")
                daemon_path = f"{{WORKDIR}}/{commit.hexsha}/output/sbin/bpfilter"
                r = executor.run(
                    f"sudo systemd-run --unit=bpfilter --slice=system.slice {daemon_path} --transient",
                    timeout=10,
                )
                if r:
                    renderer.log(f"âŒ \\[{short_sha}] Failed to start daemon")
                    executor.stats.failure(from_cache=False)
                    executor.cache[cache_key] = {"success": False}
                    renderer.update_progress()
                    continue

                time.sleep(1)

                renderer.log(f"\\[{short_sha}] Run benchmarks")
                benchmark_cmd = f"{{WORKDIR}}/{commit.hexsha}/output/sbin/benchmark_bin --cli {{WORKDIR}}/{commit.hexsha}/output/sbin/bfcli --no-daemon --daemon {{WORKDIR}}/{commit.hexsha}/output/sbin/bpfilter --srcdir {{WORKDIR}}/bpfilter --outfile {benchmark_result_path}"
                if args.wrapper:
                    benchmark_cmd = f"{args.wrapper} {benchmark_cmd}"
                benchmark_cmd = f"sudo {benchmark_cmd}"

                r = executor.run(benchmark_cmd, timeout=300)

                renderer.log(f"\\[{short_sha}] Stopping daemon")
                executor.run("sudo systemctl stop bpfilter", timeout=10)

                if r:
                    renderer.log(f"âŒ \\[{short_sha}] Run failed")
                    executor.stats.failure(from_cache=False)
                    executor.cache[cache_key] = {"success": False}
                    renderer.update_progress()
                    continue

                if not benchmark_result_path.exists():
                    renderer.log(f"âŒ \\[{short_sha}] Benchmark results not found")
                    executor.stats.failure(from_cache=False)
                    executor.cache[cache_key] = {"success": False}
                    renderer.update_progress()
                    continue

                with open(benchmark_result_path, "r") as results_file:
                    executor.cache[cache_key] = {
                        "success": True,
                        "results": json.load(results_file)["benchmarks"],
                    }
                    results.add_results(
                        [
                            Result.from_json(commit, raw)
                            for raw in executor.cache[cache_key]["results"]
                        ]
                    )

                renderer.log(f"\\[{short_sha}] Completed successfully")
                executor.stats.success(from_cache=False)
                renderer.update_progress()

            # Generate HTML report
            env = jinja2.Environment(
                loader=jinja2.FileSystemLoader(args.report_template_path.parent)
            )
            template = env.get_template(args.report_template_path.name)
            report_terms = [20]
            report_rows = renderer._prepare_benchmark_rows(
                results.sorted_benchmarks(), report_terms
            )

            with open(args.report_path, "w") as f:
                f.write(
                    template.render(
                        history=results,
                        rows=report_rows,
                        hostname=executor.host,
                        first_commit_sha=commits[0].hexsha,
                        last_commit_sha=commits[-1].hexsha,
                        n_commits=len(commits),
                        n_results=executor.stats.n_successes,
                        terms=report_terms,
                        ureg=ureg,
                        get_class=lambda stats: (
                            "neutral"
                            if not stats.is_significant
                            else "is-significant text-danger"
                            if stats.pct_change > 0
                            else "is-significant text-success"
                        ),
                    )
                )

            # Generate PR report (only significant changes) for CI comments
            if args.pr_report_output_path:
                summary_terms = [20]
                summary_template = env.get_template("summary.html.j2")
                summary_rows = renderer._prepare_benchmark_rows(
                    results.sorted_benchmarks(), summary_terms
                )

                def format_delta(s):
                    pct = f"{s.pct_change:+.2f}%"
                    if not s.is_significant:
                        return f"<code>{pct}</code>"
                    emoji = "ðŸ”´" if s.pct_change > 0 else "ðŸŸ¢"
                    return f"<code><strong>{pct}</strong></code> {emoji}"

                def has_significant_change(row):
                    for term_data in row.terms.values():
                        time_stats = term_data.get("time")
                        insn_stats = term_data.get("nInsn")
                        if (time_stats and time_stats.is_significant) or (
                            insn_stats and insn_stats.is_significant
                        ):
                            return True
                    return False

                summary_rows = [r for r in summary_rows if has_significant_change(r)]

                with open(args.pr_report_output_path, "w") as f:
                    f.write(
                        summary_template.render(
                            rows=summary_rows,
                            terms=summary_terms,
                            stats=executor.stats,
                            format_delta=format_delta,
                        )
                    )

    if args.no_tui:
        run(executor)
    else:
        with rich.live.Live(
            renderer.layout,
            console=renderer.console,
            refresh_per_second=10,
            screen=True,
        ):
            run(executor)

    # Summary
    renderer.print_summary(executor.stats, results.sorted_benchmarks())

    return results


def main():
    parser = argparse.ArgumentParser(
        prog="bfbencher",
        description="",
    )

    parser.add_argument(
        "--since",
        type=str,
        help=f'Oldest commit to benchmark. Use "wip" to start from the uncommitted changes (committed as "bfbencher: WIP"). Must be older than --until, or the same. Defaults to "{DEFAULT_FIRST_COMMIT_REF}"',
        default=DEFAULT_FIRST_COMMIT_REF,
    )
    parser.add_argument(
        "--include",
        type=str,
        action="append",
        default=[],
        help='Include an extra commit outside the range. Can be specified multiple times. Use "wip" to include uncommitted changes. Commits are sorted in git order with the range commits.',
    )
    parser.add_argument(
        "--until",
        type=str,
        help=f'Newest commit to benchmark. Use "wip" to include uncommitted changes (committed as "bfbencher: WIP"). Must be newer than --since, or the same. Defaults to "{DEFAULT_LAST_COMMIT_REF}"',
        default=DEFAULT_LAST_COMMIT_REF,
    )
    parser.add_argument(
        "--sources",
        type=pathlib.Path,
        help=f'path to the bpfilter sources directory. Defaults to "{DEFAULT_SOURCE_PATH}".',
        default=DEFAULT_SOURCE_PATH,
    )
    parser.add_argument(
        "--host",
        type=str,
        help=f'host to run the benchmark on. bfbencher will connect to the host using SSH, copy the project sources on it, and run the benchmarks. Defaults to "{DEFAULT_HOST[0]}" (current host).',
        default=DEFAULT_HOST[0],
    )
    parser.add_argument(
        "--cache-dir",
        type=pathlib.Path,
        help=f"path to the directory containing the cached results. The cache is used to store benchmark results based on the hostname and the commit SHA, it is stored on the host running bfbencher. Defaults to {DEFAULT_CACHE_PATH}.",
        default=DEFAULT_CACHE_PATH,
    )
    parser.add_argument(
        "--report-template-path",
        type=pathlib.Path,
        help=f'path to the Jinja2 template use to generate the HTML report. Defaults to "{DEFAULT_REPORT_TEMPLATE_PATH}"',
        default=DEFAULT_REPORT_TEMPLATE_PATH,
    )
    parser.add_argument(
        "--report-path",
        type=pathlib.Path,
        help=f'path of the final HTML report. Defaults to "{DEFAULT_REPORT_PATH}".',
        default=DEFAULT_REPORT_PATH,
    )
    parser.add_argument(
        "--retry",
        "-r",
        type=str,
        action="append",
        default=[],
        help='retry benchmarks for specific commits, ignoring cached results. Use "failed" to retry all failed commits, "all" to retry everything, or a commit ref to retry a specific commit. Can be specified multiple times.',
    )
    parser.add_argument(
        "--no-tui",
        action="store_true",
        help="if set, print to the console without TUI",
        default=False,
    )
    parser.add_argument(
        "--pr-report-output-path",
        type=pathlib.Path,
        help="path to write the PR report (HTML summary showing only statistically significant changes, for CI comments)",
        default=None,
    )
    parser.add_argument(
        "--fail-on-significant-change",
        choices=["better", "worse", "any"],
        help="exit with non-zero status if any benchmark has a statistically significant change (better=improvement, worse=regression, any=either)",
        default=None,
    )
    parser.add_argument(
        "--wrapper",
        type=str,
        help="wrapper command to prepend to the benchmark execution (e.g., 'taskset -c 0' or 'numactl --cpunodebind=0')",
        default=None,
    )
    parser.add_argument(
        "--lock-file",
        type=pathlib.Path,
        help="path to a lock file to hold while running benchmarks (prevents concurrent runs)",
        default=None,
    )
    parser.add_argument(
        "--lock-wait-for",
        type=int,
        help="seconds to wait for lock acquisition before exiting (default: 0, exit immediately)",
        default=0,
    )

    args = parser.parse_args()

    global renderer
    renderer = ConsoleRenderer() if args.no_tui else TuiRenderer()

    try:
        results = run_benchmarks(args)
    except KeyboardInterrupt:
        renderer.log("Command interrupted by user")
        raise SystemExit(1)

    if args.fail_on_significant_change:
        terms = [5, 15]
        for benchmark in results.sorted_benchmarks():
            if has_significant_change(
                benchmark, terms, args.fail_on_significant_change
            ):
                raise SystemExit(1)


if __name__ == "__main__":
    main()
